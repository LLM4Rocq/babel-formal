python -m torch.distributed.run --nproc-per-node 3 --master_port 12345 src/training/training.py \
    --block_size=4096 \
    --per_device_train_batch_size=1 \
    --per_device_eval_batch_size=1 \
    --gradient_accumulation_steps=6 \
    --num_train_epochs=5 \
    --model_name="Qwen/Qwen2.5-1.5B-Instruct" \
    --warmup_ratio=0.05 \
    --fsdp="full_shard auto_wrap" \
    --fsdp_config="src/training/config/fsdp_config_qwen.json" \
    --bf16=True \
    --eval_strategy="no" \
    --logging_steps=1 \
    --save_strategy="no" \
    --lr_scheduler_type="cosine" \
    --learning_rate=1e-5 \
    --weight_decay=1e-4 \
    --adam_beta1=0.9 \
    --adam_beta2=0.95 \
    --output_dir="export/training" \
    --save_only_model=True \
    --gradient_checkpointing=True \
    --eval_pass_k=32
